{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task1 - 速通 Baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-08T12:50:49.017025Z",
     "iopub.status.busy": "2024-07-08T12:50:49.016753Z",
     "iopub.status.idle": "2024-07-08T12:50:50.972508Z",
     "shell.execute_reply": "2024-07-08T12:50:50.971857Z",
     "shell.execute_reply.started": "2024-07-08T12:50:49.017005Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-08T12:50:50.974203Z",
     "iopub.status.busy": "2024-07-08T12:50:50.973877Z",
     "iopub.status.idle": "2024-07-08T12:50:52.080997Z",
     "shell.execute_reply": "2024-07-08T12:50:52.080232Z",
     "shell.execute_reply.started": "2024-07-08T12:50:50.974178Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Miniconda\\miniconda3\\envs\\mt\\lib\\site-packages\\torchtext\\data\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "import random\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T12:50:52.082249Z",
     "iopub.status.busy": "2024-07-08T12:50:52.081911Z",
     "iopub.status.idle": "2024-07-08T12:50:52.092084Z",
     "shell.execute_reply": "2024-07-08T12:50:52.091346Z",
     "shell.execute_reply.started": "2024-07-08T12:50:52.082230Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义数据集类\n",
    "# 修改TranslationDataset类以处理术语\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, filename, terminology):\n",
    "        self.data = []\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                en, zh = line.strip().split('\\t')\n",
    "                self.data.append((en, zh))\n",
    "        \n",
    "        self.terminology = terminology\n",
    "        \n",
    "        # 创建词汇表，注意这里需要确保术语词典中的词也被包含在词汇表中\n",
    "        self.en_tokenizer = get_tokenizer('basic_english')\n",
    "        self.zh_tokenizer = list  # 使用字符级分词\n",
    "        \n",
    "        en_vocab = Counter(self.terminology.keys())  # 确保术语在词汇表中\n",
    "        zh_vocab = Counter()\n",
    "        \n",
    "        for en, zh in self.data:\n",
    "            en_vocab.update(self.en_tokenizer(en))\n",
    "            zh_vocab.update(self.zh_tokenizer(zh))\n",
    "        \n",
    "        # 添加术语到词汇表\n",
    "        self.en_vocab = ['<pad>', '<sos>', '<eos>'] + list(self.terminology.keys()) + [word for word, _ in en_vocab.most_common(10000)]\n",
    "        self.zh_vocab = ['<pad>', '<sos>', '<eos>'] + [word for word, _ in zh_vocab.most_common(10000)]\n",
    "        \n",
    "        self.en_word2idx = {word: idx for idx, word in enumerate(self.en_vocab)}\n",
    "        self.zh_word2idx = {word: idx for idx, word in enumerate(self.zh_vocab)}\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        en, zh = self.data[idx]\n",
    "        en_tensor = torch.tensor([self.en_word2idx.get(word, self.en_word2idx['<sos>']) for word in self.en_tokenizer(en)] + [self.en_word2idx['<eos>']])\n",
    "        zh_tensor = torch.tensor([self.zh_word2idx.get(word, self.zh_word2idx['<sos>']) for word in self.zh_tokenizer(zh)] + [self.zh_word2idx['<eos>']])\n",
    "        return en_tensor, zh_tensor\n",
    "\n",
    "def collate_fn(batch):\n",
    "    en_batch, zh_batch = [], []\n",
    "    for en_item, zh_item in batch:\n",
    "        en_batch.append(en_item)\n",
    "        zh_batch.append(zh_item)\n",
    "    \n",
    "    # 对英文和中文序列分别进行填充\n",
    "    en_batch = nn.utils.rnn.pad_sequence(en_batch, padding_value=0, batch_first=True)\n",
    "    zh_batch = nn.utils.rnn.pad_sequence(zh_batch, padding_value=0, batch_first=True)\n",
    "    \n",
    "    return en_batch, zh_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T12:50:52.093405Z",
     "iopub.status.busy": "2024-07-08T12:50:52.093120Z",
     "iopub.status.idle": "2024-07-08T12:50:52.102203Z",
     "shell.execute_reply": "2024-07-08T12:50:52.101497Z",
     "shell.execute_reply.started": "2024-07-08T12:50:52.093388Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src shape: [batch_size, src_len]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded shape: [batch_size, src_len, emb_dim]\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        # outputs shape: [batch_size, src_len, hid_dim]\n",
    "        # hidden shape: [n_layers, batch_size, hid_dim]\n",
    "        return outputs, hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # input shape: [batch_size, 1]\n",
    "        # hidden shape: [n_layers, batch_size, hid_dim]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # embedded shape: [batch_size, 1, emb_dim]\n",
    "        \n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        # output shape: [batch_size, 1, hid_dim]\n",
    "        # hidden shape: [n_layers, batch_size, hid_dim]\n",
    "        \n",
    "        prediction = self.fc_out(output.squeeze(1))\n",
    "        # prediction shape: [batch_size, output_dim]\n",
    "        \n",
    "        return prediction, hidden\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # src shape: [batch_size, src_len]\n",
    "        # trg shape: [batch_size, trg_len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        _, hidden = self.encoder(src)\n",
    "        \n",
    "        input = trg[:, 0].unsqueeze(1)  # Start token\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input, hidden)\n",
    "            outputs[:, t, :] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[:, t].unsqueeze(1) if teacher_force else top1.unsqueeze(1)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新增术语词典加载部分\n",
    "def load_terminology_dictionary(dict_file):\n",
    "    terminology = {}\n",
    "    with open(dict_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            en_term, ch_term = line.strip().split('\\t')\n",
    "            terminology[en_term] = ch_term\n",
    "    return terminology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T12:50:52.103387Z",
     "iopub.status.busy": "2024-07-08T12:50:52.103142Z",
     "iopub.status.idle": "2024-07-08T12:50:52.107862Z",
     "shell.execute_reply": "2024-07-08T12:50:52.107237Z",
     "shell.execute_reply.started": "2024-07-08T12:50:52.103369Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, (src, trg) in enumerate(iterator):\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:, 1:].contiguous().view(-1, output_dim)\n",
    "        trg = trg[:, 1:].contiguous().view(-1)\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-08T12:50:52.108867Z",
     "iopub.status.busy": "2024-07-08T12:50:52.108637Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 主函数\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()  # 开始计时\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    #terminology = load_terminology_dictionary('../dataset/en-zh.dic')\n",
    "    terminology = load_terminology_dictionary('../dataset/en-zh.dic')\n",
    "\n",
    "    # 加载数据\n",
    "    dataset = TranslationDataset('../dataset/train.txt',terminology = terminology)\n",
    "    # 选择数据集的前N个样本进行训练\n",
    "    N = 100000  #int(len(dataset) * 1)  # 或者你可以设置为数据集大小的一定比例，如 int(len(dataset) * 0.1)\n",
    "    subset_indices = list(range(N))\n",
    "    subset_dataset = Subset(dataset, subset_indices)\n",
    "    train_loader = DataLoader(subset_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    # 定义模型参数\n",
    "    INPUT_DIM = len(dataset.en_vocab)\n",
    "    OUTPUT_DIM = len(dataset.zh_vocab)\n",
    "    ENC_EMB_DIM = 256\n",
    "    DEC_EMB_DIM = 256\n",
    "    HID_DIM = 512\n",
    "    N_LAYERS = 2\n",
    "    ENC_DROPOUT = 0.5\n",
    "    DEC_DROPOUT = 0.5\n",
    "\n",
    "    # 初始化模型\n",
    "    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "    model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "    # 定义优化器和损失函数\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=dataset.zh_word2idx['<pad>'])\n",
    "\n",
    "    # 训练模型\n",
    "    N_EPOCHS = 10\n",
    "    CLIP = 1\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
    "        print(f'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f}')\n",
    "        \n",
    "    # 在训练循环结束后保存模型\n",
    "    torch.save(model.state_dict(), './translation_model_GRU.pth')\n",
    "    \n",
    "    end_time = time.time()  # 结束计时\n",
    "\n",
    "    # 计算并打印运行时间\n",
    "    elapsed_time_minute = (end_time - start_time)/60\n",
    "    print(f\"Total running time: {elapsed_time_minute:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 在开发集上进行模型评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sacrebleu.metrics import BLEU\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设我们已经定义了TranslationDataset, Encoder, Decoder, Seq2Seq类\n",
    "\n",
    "def load_sentences(file_path: str) -> List[str]:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return [line.strip() for line in f]\n",
    "\n",
    "# 更新translate_sentence函数以考虑术语词典\n",
    "def translate_sentence(sentence: str, model: Seq2Seq, dataset: TranslationDataset, terminology, device: torch.device, max_length: int = 50):\n",
    "    model.eval()\n",
    "    tokens = dataset.en_tokenizer(sentence)\n",
    "    tensor = torch.LongTensor([dataset.en_word2idx.get(token, dataset.en_word2idx['<sos>']) for token in tokens]).unsqueeze(0).to(device)  # [1, seq_len]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _, hidden = model.encoder(tensor)\n",
    "\n",
    "    translated_tokens = []\n",
    "    input_token = torch.LongTensor([[dataset.zh_word2idx['<sos>']]]).to(device)  # [1, 1]\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        output, hidden = model.decoder(input_token, hidden)\n",
    "        top_token = output.argmax(1)\n",
    "        translated_token = dataset.zh_vocab[top_token.item()]\n",
    "        \n",
    "        if translated_token == '<eos>':\n",
    "            break\n",
    "        \n",
    "        # 如果翻译的词在术语词典中，则使用术语词典中的词\n",
    "        if translated_token in terminology.values():\n",
    "            for en_term, ch_term in terminology.items():\n",
    "                if translated_token == ch_term:\n",
    "                    translated_token = en_term\n",
    "                    break\n",
    "        \n",
    "        translated_tokens.append(translated_token)\n",
    "        input_token = top_token.unsqueeze(1)  # [1, 1]\n",
    "\n",
    "    return ''.join(translated_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bleu(model: Seq2Seq, dataset: TranslationDataset, src_file: str, ref_file: str, terminology,device: torch.device):\n",
    "    model.eval()\n",
    "    src_sentences = load_sentences(src_file)\n",
    "    ref_sentences = load_sentences(ref_file)\n",
    "    \n",
    "    translated_sentences = []\n",
    "    for src in src_sentences:\n",
    "        translated = translate_sentence(src, model, dataset, terminology, device)\n",
    "        translated_sentences.append(translated)\n",
    "    \n",
    "    bleu = BLEU()\n",
    "    score = bleu.corpus_score(translated_sentences, [ref_sentences])\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主函数\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # 加载术语词典\n",
    "    terminology = load_terminology_dictionary('../dataset/en-zh.dic')\n",
    "    \n",
    "    # 创建数据集实例时传递术语词典\n",
    "    dataset = TranslationDataset('../dataset/train.txt', terminology)\n",
    "    \n",
    "\n",
    "    # 定义模型参数\n",
    "    INPUT_DIM = len(dataset.en_vocab)\n",
    "    OUTPUT_DIM = len(dataset.zh_vocab)\n",
    "    ENC_EMB_DIM = 256\n",
    "    DEC_EMB_DIM = 256\n",
    "    HID_DIM = 512\n",
    "    N_LAYERS = 2\n",
    "    ENC_DROPOUT = 0.5\n",
    "    DEC_DROPOUT = 0.5\n",
    "\n",
    "    # 初始化模型\n",
    "    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "    model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "    # 加载训练好的模型\n",
    "    model.load_state_dict(torch.load('./translation_model_GRU.pth'))\n",
    "\n",
    "    # 评估BLEU分数\n",
    "    bleu_score = evaluate_bleu(model, dataset, '../dataset/dev_en.txt', '../dataset/dev_zh.txt', terminology = terminology,device = device)\n",
    "    print(f'BLEU-4 score: {bleu_score.score:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 在测试集上进行推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model: Seq2Seq, dataset: TranslationDataset, src_file: str, save_dir:str, terminology, device: torch.device):\n",
    "    model.eval()\n",
    "    src_sentences = load_sentences(src_file)\n",
    "    \n",
    "    translated_sentences = []\n",
    "    for src in src_sentences:\n",
    "        translated = translate_sentence(src, model, dataset, terminology, device)\n",
    "        #print(translated)\n",
    "        translated_sentences.append(translated)\n",
    "        #print(translated_sentences)\n",
    "\n",
    "    # 将列表元素连接成一个字符串，每个元素后换行\n",
    "    text = '\\n'.join(translated_sentences)\n",
    "\n",
    "    # 打开一个文件，如果不存在则创建，'w'表示写模式\n",
    "    with open(save_dir, 'w', encoding='utf-8') as f:\n",
    "        # 将字符串写入文件\n",
    "        f.write(text)\n",
    "\n",
    "    #return translated_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "翻译完成！文件已保存到../dataset/submit.txt\n"
     ]
    }
   ],
   "source": [
    "# 主函数\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # 加载术语词典\n",
    "    terminology = load_terminology_dictionary('../dataset/en-zh.dic')\n",
    "    # 加载数据集和模型\n",
    "    dataset = TranslationDataset('../dataset/train.txt',terminology = terminology)\n",
    "\n",
    "    # 定义模型参数\n",
    "    INPUT_DIM = len(dataset.en_vocab)\n",
    "    OUTPUT_DIM = len(dataset.zh_vocab)\n",
    "    ENC_EMB_DIM = 256\n",
    "    DEC_EMB_DIM = 256\n",
    "    HID_DIM = 512\n",
    "    N_LAYERS = 2\n",
    "    ENC_DROPOUT = 0.5\n",
    "    DEC_DROPOUT = 0.5\n",
    "\n",
    "    # 初始化模型\n",
    "    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "    model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "    # 加载训练好的模型\n",
    "    model.load_state_dict(torch.load('./translation_model_GRU.pth'))\n",
    "    \n",
    "    save_dir = '../dataset/submit.txt'\n",
    "    inference(model, dataset, src_file=\"../dataset/test_en.txt\", save_dir = save_dir, terminology = terminology, device = device)\n",
    "    print(f\"翻译完成！文件已保存到{save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
